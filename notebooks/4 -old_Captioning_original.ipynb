{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Image Captioning\n\n#### Goals:\n- Caption images using computer vision and NLP.\n- Test caption similarities to real captions using BLEU score.\n\nThis work is inspired by the following article: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/#:~:text=deep%20learning%20model.-,Defining%20the%20Model,-We%20will%20define\n\n\n### Import Libraries",
   "metadata": {
    "tags": [],
    "cell_id": "00000-37a4cb7a-f6d0-46f0-8754-758bb7699491",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1bf518ae",
    "execution_start": 1637883121411,
    "execution_millis": 3983,
    "cell_id": "00001-ffd72bef-6743-42cb-8523-7d3cc88de73c",
    "deepnote_cell_type": "code"
   },
   "source": "from pickle import load\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom numpy import argmax\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Import Data",
   "metadata": {
    "tags": [],
    "cell_id": "00002-de3305a1-f977-414e-b298-3ed55d89e6be",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2934c33",
    "execution_start": 1637883170386,
    "execution_millis": 18,
    "cell_id": "00003-6d9b5a17-e5fa-4fd3-b398-ae8cd2d7fd01",
    "deepnote_cell_type": "code"
   },
   "source": "frontal_train = pd.read_pickle(\"../data/train/frontal_train.pickle\").reset_index(drop=True)\n\nroot_dir = \"/datasets/gdrive/XRay-AKAKI/images_normalized/\"\n\nfrontal_test = pd.read_pickle('../data/test/frontal_test.pickle').reset_index(drop=True)\ntest_descs = frontal_test['caption']\n\n#Randomly select 20 x-ray images for train and 20 for testing\ntrain = frontal_train.sample(20)\ntest = frontal_test.sample(20)\n\ntrain_path = train['filename']\ntest_path = test['filename']\n\ntrain_descs = train['caption']\ntest_descs = test['caption']",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating Utility Functions",
   "metadata": {
    "tags": [],
    "cell_id": "00006-bc47a87a-2aee-446b-a587-e8dc09030a1e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ba917b1b",
    "execution_start": 1637883173735,
    "execution_millis": 2,
    "cell_id": "00007-a098158d-aece-4026-b612-e5a02dc0f50c",
    "deepnote_cell_type": "code"
   },
   "source": "# extract features from each photo in the directory\ndef extract_features(filenames):\n    \"\"\"\n    Function to extract the features of the images using VGG16.\n    \"\"\"\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # summarize\n    print(model.summary())\n    # extract features from each photo\n    features = dict()\n    for filename in filenames:\n        # load an image from file\n        image = load_img(root_dir+filename, target_size=(224, 224))\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        # prepare the image for the VGG model\n        image = preprocess_input(image)\n        # get features\n        feature = model.predict(image, verbose=0)\n        # get image id\n        image_id = filename.split('.')[0]\n        # store feature\n        features[image_id] = feature\n        print('>%s' % filename)\n    return features",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "363af1e3",
    "execution_start": 1637883140514,
    "execution_millis": 0,
    "cell_id": "00008-43a68714-6ee0-48f2-b097-0d9c230abf71",
    "deepnote_cell_type": "code"
   },
   "source": "def create_tokenizer(descriptions):\n    \"\"\"\n    Function to generate tokenizer by fitting on descriptions.\n    \"\"\"\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(descriptions)\n    return tokenizer\n\n\ndef to_max_length(lines):\n    \"\"\"\n    Function to calculate the length of the descriptions with the most words.\n    \"\"\"\n    \n    return max(len(d.split()) for d in lines)\n\n\ndef create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n    \"\"\"\n    Create sequences of images, input sequences and output words for an image\n    \"\"\"\n    X1, X2, y = list(), list(), list()\n    # walk through each image identifier\n    for key, desc_list in descriptions.items():\n        # walk through each description for the image\n        for desc in desc_list:\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc])[0]\n            # split one sequence into multiple X,y pairs\n            for i in range(1, len(seq)):\n                # split into input and output pair\n                in_seq, out_seq = seq[:i], seq[i]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                X1.append(photos[key][0])\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7f8224f3",
    "execution_start": 1637883177346,
    "execution_millis": 2,
    "cell_id": "00009-fdd8df42-6f09-4117-9484-f08da251fc1e",
    "deepnote_cell_type": "code"
   },
   "source": "def define_model(vocab_size, max_length):\n    \"\"\"\n    Define the captioning model.\n    \"\"\"\n    # feature extractor model\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    # decoder model\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ea1f3f2e",
    "execution_start": 1637883182521,
    "execution_millis": 66499,
    "cell_id": "00010-244a17dc-d014-4ec2-b197-ea2726b0c9a8",
    "deepnote_cell_type": "code"
   },
   "source": "#Getting the length of the longest word\nmaxx = to_max_length(train_descs)\n\n#Creating the tokenizer\ntokenizer = create_tokenizer(train_descs)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n\n#Extracting the features\ntrain_features = extract_features(train_path)\ntest_features = extract_features(test_path)\nprint(\"Extracted features.\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Vocabulary Size: 228\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\n>2149_IM-0768-0001-0001.dcm.png\n>3212_IM-1517-1001.dcm.png\n>3282_IM-1563-2001.dcm.png\n>2151_IM-0771-1001.dcm.png\n>3147_IM-1480-1001.dcm.png\n>358_IM-1759-1001.dcm.png\n>3318_IM-1587-1001.dcm.png\n>3303_IM-1580-3001.dcm.png\n>853_IM-2375-1001.dcm.png\n>2688_IM-1159-4004.dcm.png\n>939_IM-2435-1001.dcm.png\n>715_IM-2277-2001.dcm.png\n>1405_IM-0259-1001.dcm.png\n>304_IM-1413-12012.dcm.png\n>2069_IM-0702-1001.dcm.png\n>1758_IM-0495-1001.dcm.png\n>1539_IM-0349-1001.dcm.png\n>2052_IM-0690-1001.dcm.png\n>2456_IM-0989-1001.dcm.png\n>3629_IM-1797-1001.dcm.png\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\n>842_IM-2366-1001.dcm.png\n>2661_IM-1142-1001.dcm.png\n>1825_IM-0535-1001.dcm.png\n>2959_IM-1354-1001.dcm.png\n>865_IM-2385-1001.dcm.png\n>1919_IM-0598-1001.dcm.png\n>394_IM-2010-1001.dcm.png\n>3582_IM-1761-1001.dcm.png\n>1079_IM-0056-1001.dcm.png\n>1570_IM-0372-1001.dcm.png\n>3847_IM-1946-1001.dcm.png\n>1800_IM-0520-1001.dcm.png\n>2467_IM-0997-1001.dcm.png\n>907_IM-2412-82526002.dcm.png\n>278_IM-1218-1001.dcm.png\n>3142_IM-1477-2001.dcm.png\n>869_IM-2389-2001.dcm.png\n>1957_IM-0624-4004.dcm.png\n>1447_IM-0289-1001.dcm.png\n>3473_IM-1688-2002.dcm.png\nExtracted features.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d4a81f72",
    "execution_start": 1637878352094,
    "execution_millis": 53,
    "cell_id": "00014-c847aea5-46ac-428f-8042-53ddeaa2f41a",
    "deepnote_cell_type": "code"
   },
   "source": "#prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, maxx, train_descs, train_features, vocab_size)\nprint(\"Created sequences for train.\")\n\n# prepare sequences\nX1test, X2test, ytest = create_sequences(tokenizer, maxx, test_descs, test_features, vocab_size)\nprint(\"Created sequences for test.\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Created sequences for train.\nCreated sequences for test.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c1cdcc9c",
    "execution_start": 1637878315122,
    "execution_millis": 973,
    "cell_id": "00015-37a9c16c-e1e6-4aee-8096-bc7c472b3599",
    "deepnote_cell_type": "code"
   },
   "source": "# define the model\nmodel = define_model(vocab_size, maxx)\n# define checkpoint callback\nfilepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# fit model\nmodel.fit([X1train, X2train], ytrain, epochs=5, verbose=1, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"model_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_10 (InputLayer)           [(None, 101)]        0                                            \n__________________________________________________________________________________________________\ninput_9 (InputLayer)            [(None, 4096)]       0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 101, 256)     54784       input_10[0][0]                   \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 4096)         0           input_9[0][0]                    \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 101, 256)     0           embedding_3[0][0]                \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 256)          1048832     dropout_6[0][0]                  \n__________________________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 256)          525312      dropout_7[0][0]                  \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 256)          0           dense_9[0][0]                    \n                                                                 lstm_3[0][0]                     \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 256)          65792       add_3[0][0]                      \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 214)          54998       dense_10[0][0]                   \n==================================================================================================\nTotal params: 1,749,718\nTrainable params: 1,749,718\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Generate the Descriptions",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00018-4d7ac630-f1f6-4140-ac88-24b1853369ad",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8bb547f7",
    "execution_start": 1637514637447,
    "execution_millis": 22,
    "cell_id": "00019-3e6d3af0-8c2a-4df2-ab08-dc7000eaba3a",
    "deepnote_cell_type": "code"
   },
   "source": "# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('model-ep002-loss3.245-val_loss3.612.h5')\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tokenizer.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a1b05dfeda2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m# pre-define the max sequence length (from training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tokenizer.pkl'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00020-d041c899-216c-4e96-9543-4bc1a8342399",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d67d5ca8-f99a-4baf-8748-4aa99efbd09b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "4c7d27de-9756-4001-aeca-4590ff924e2e",
  "deepnote_execution_queue": []
 }
}