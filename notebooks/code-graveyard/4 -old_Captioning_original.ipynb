{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Image Captioning\n\n#### Goals:\n- Caption images using computer vision and NLP.\n- Test caption similarities to real captions using BLEU score.\n\nThis work is inspired by the following article: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/#:~:text=deep%20learning%20model.-,Defining%20the%20Model,-We%20will%20define\n\n\n### Import Libraries",
   "metadata": {
    "tags": [],
    "cell_id": "00000-37a4cb7a-f6d0-46f0-8754-758bb7699491",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1bf518ae",
    "execution_start": 1639426719736,
    "execution_millis": 3819,
    "cell_id": "00001-ffd72bef-6743-42cb-8523-7d3cc88de73c",
    "deepnote_cell_type": "code"
   },
   "source": "from pickle import load\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom numpy import argmax\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Import Data",
   "metadata": {
    "tags": [],
    "cell_id": "00002-de3305a1-f977-414e-b298-3ed55d89e6be",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "660ab636",
    "execution_start": 1639426723559,
    "execution_millis": 19,
    "cell_id": "00003-6d9b5a17-e5fa-4fd3-b398-ae8cd2d7fd01",
    "deepnote_cell_type": "code"
   },
   "source": "frontal_train = pd.read_pickle(\"../data/train/frontal_train.pickle\").reset_index(drop=True)\n\nroot_dir = \"/datasets/gdrive/XRay-AKAKI/images_normalized/\"\n\nfrontal_test = pd.read_pickle('../data/test/frontal_test.pickle').reset_index(drop=True)\ntest_descs = frontal_test['caption']\n\n#Randomly select 20 x-ray images for train and 20 for testing\ntrain = frontal_train.sample(2)\ntest = frontal_test.sample(2)\n\ntrain_path = train['filename']\ntest_path = test['filename']\n\ntrain_descs = train['caption']\ntest_descs = test['caption']",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating Utility Functions",
   "metadata": {
    "tags": [],
    "cell_id": "00006-bc47a87a-2aee-446b-a587-e8dc09030a1e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ba917b1b",
    "execution_start": 1639426723585,
    "execution_millis": 44,
    "cell_id": "00007-a098158d-aece-4026-b612-e5a02dc0f50c",
    "deepnote_cell_type": "code"
   },
   "source": "# extract features from each photo in the directory\ndef extract_features(filenames):\n    \"\"\"\n    Function to extract the features of the images using VGG16.\n    \"\"\"\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # summarize\n    print(model.summary())\n    # extract features from each photo\n    features = dict()\n    for filename in filenames:\n        # load an image from file\n        image = load_img(root_dir+filename, target_size=(224, 224))\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        # prepare the image for the VGG model\n        image = preprocess_input(image)\n        # get features\n        feature = model.predict(image, verbose=0)\n        # get image id\n        image_id = filename.split('.')[0]\n        # store feature\n        features[image_id] = feature\n        print('>%s' % filename)\n    return features",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d9c89a8d",
    "execution_start": 1639426724196,
    "execution_millis": 2,
    "cell_id": "00008-43a68714-6ee0-48f2-b097-0d9c230abf71",
    "deepnote_cell_type": "code"
   },
   "source": "def create_tokenizer(descriptions):\n    \"\"\"\n    Function to generate tokenizer by fitting on descriptions.\n    \"\"\"\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(descriptions)\n    return tokenizer\n\n\ndef to_max_length(lines):\n    \"\"\"\n    Function to calculate the length of the descriptions with the most words.\n    \"\"\"\n    \n    return max(len(d.split()) for d in lines)\n\n\ndef create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n    \"\"\"\n    Create sequences of images, input sequences and output words for an image\n    \"\"\"\n    X1, X2, y = list(), list(), list()\n    # walk through each image identifier\n    for key, desc_list in descriptions.items():\n        print(f\"key: {key}\")\n        print(f\"value: {desc_list}\")\n        # walk through each description for the image\n        for desc in desc_list:\n            print(f\"desc: {desc}\")\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc])[0]\n            # split one sequence into multiple X,y pairs\n            for i in range(1, len(seq)):\n                # split into input and output pair\n                in_seq, out_seq = seq[:i], seq[i]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                X1.append(photos[key][0])\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-9d5d1648-de06-44cc-8d60-78407d8da227",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a9034863",
    "execution_start": 1639426725284,
    "execution_millis": 187,
    "deepnote_cell_type": "code"
   },
   "source": "#prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, maxx, train_descs, train_features, vocab_size)\nprint(\"Created sequences for train.\")",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cd90b11df888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#prepare sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX1train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_descs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created sequences for train.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ],
     "data": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-95625b03-539f-4301-ae1a-f969d315b41b",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7f8224f3",
    "execution_start": 1639426486408,
    "execution_millis": 7,
    "cell_id": "00009-fdd8df42-6f09-4117-9484-f08da251fc1e",
    "deepnote_cell_type": "code"
   },
   "source": "def define_model(vocab_size, max_length):\n    \"\"\"\n    Define the captioning model.\n    \"\"\"\n    # feature extractor model\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    # decoder model\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ea1f3f2e",
    "execution_start": 1639426487305,
    "execution_millis": 9107,
    "cell_id": "00010-244a17dc-d014-4ec2-b197-ea2726b0c9a8",
    "deepnote_cell_type": "code"
   },
   "source": "#Getting the length of the longest word\nmaxx = to_max_length(train_descs)\n\n#Creating the tokenizer\ntokenizer = create_tokenizer(train_descs)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n\n#Extracting the features\ntrain_features = extract_features(train_path)\ntest_features = extract_features(test_path)\nprint(\"Extracted features.\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Vocabulary Size: 44\nModel: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\nWARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa36e56b050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n>1470_IM-0303-1001.dcm.png\n>2771_IM-1213-1001.dcm.png\n",
     "output_type": "stream",
     "data": {}
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ],
     "data": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d4a81f72",
    "execution_start": 1639426481119,
    "execution_millis": 18,
    "cell_id": "00014-c847aea5-46ac-428f-8042-53ddeaa2f41a",
    "deepnote_cell_type": "code"
   },
   "source": "#prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, maxx, train_descs, train_features, vocab_size)\nprint(\"Created sequences for train.\")\n\n# prepare sequences\nX1test, X2test, ytest = create_sequences(tokenizer, maxx, test_descs, test_features, vocab_size)\nprint(\"Created sequences for test.\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Created sequences for train.\nCreated sequences for test.\n",
     "output_type": "stream",
     "data": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-18143aeb-c92a-4eb7-985d-37b76c3cf7f7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1e2c9114",
    "execution_start": 1639426166098,
    "execution_millis": 6,
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_cell_type": "code"
   },
   "source": "X1train",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "data": {
      "text/plain": "array([], dtype=float64)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8a1d8fe5",
    "execution_start": 1639426142259,
    "execution_millis": 1160,
    "cell_id": "00015-37a9c16c-e1e6-4aee-8096-bc7c472b3599",
    "deepnote_cell_type": "code"
   },
   "source": "# define the model\nmodel = define_model(vocab_size, maxx)\n# define checkpoint callback\nfilepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# fit model\nmodel.fit([X1train, X2train], ytrain, epochs=5, verbose=1, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 40)]         0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 4096)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 40, 256)      11264       input_4[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 4096)         0           input_3[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 40, 256)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 44)           11308       dense_1[0][0]                    \n==================================================================================================\nTotal params: 1,662,508\nTrainable params: 1,662,508\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/5\n",
     "output_type": "stream",
     "data": {}
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-16aeefbb358c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ],
     "data": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Generate the Descriptions",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00018-4d7ac630-f1f6-4140-ac88-24b1853369ad",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8bb547f7",
    "execution_start": 1637514637447,
    "execution_millis": 22,
    "cell_id": "00019-3e6d3af0-8c2a-4df2-ab08-dc7000eaba3a",
    "deepnote_cell_type": "code"
   },
   "source": "# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('model-ep002-loss3.245-val_loss3.612.h5')\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tokenizer.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a1b05dfeda2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m# pre-define the max sequence length (from training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tokenizer.pkl'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00020-d041c899-216c-4e96-9543-4bc1a8342399",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test stuff with albion",
   "metadata": {
    "tags": [],
    "cell_id": "00017-4c8114d9-a52c-4ddc-ada8-07091c74c0c0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-774d064e-7a26-44c5-a3c0-aebcea157bca",
    "deepnote_cell_type": "code"
   },
   "source": "def define_model(vocab_size, max_length):\n    \"\"\"\n    Define the captioning model.\n    \"\"\"\n    # feature extractor model\n    inputs1 = Input(shape=(4096,))\n    #encoder_output = Densenet_model(inputs1)   \n    fe2 = Flatten()(inputs1)\n    fe3 = Dropout(0.5)(fe2)\n    fe4 = Dense(256, activation='relu')(fe3)\n    fe5 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(fe4)\n\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n \n    # decoder model\n    decoder1 = add([fe5, se2])\n    decoder2 = LSTM(256)(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n\n\n\n    ## Albion's modification\n\ndef create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n    \"\"\"\n    Create sequences of images, input sequences and output words for an image (this cell may be an issue)\n    \"\"\"\n    X1, X2, y = list(), list(), list()\n    # walk through each image identifier\n    for key, desc_list in descriptions.items():\n        print(f\"key: {key}\")\n        print(f\"value: {desc_list}\")\n        # walk through each description for the image\n        for desc in descriptions:\n            print(f\"desc: {desc}\")\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc])[0]\n            # split one sequence into multiple X,y pairs\n            for i in range(1, len(seq)):\n                # split into input and output pair\n                in_seq, out_seq = seq[:i], seq[i]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                X1.append(photos[key][0])\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)\n\n\n## the orignial one\n\ndef create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n    \"\"\"\n    Create sequences of images, input sequences and output words for an image\n    \"\"\"\n    X1, X2, y = list(), list(), list()\n    # walk through each image identifier\n    for key, desc_list in descriptions.items():\n        print(f\"key: {key}\")\n        print(f\"value: {desc_list}\")\n        # walk through each description for the image\n        for desc in desc_list:\n            print(f\"desc: {desc}\")\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc])[0]\n            # split one sequence into multiple X,y pairs\n            for i in range(1, len(seq)):\n                # split into input and output pair\n                in_seq, out_seq = seq[:i], seq[i]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                X1.append(photos[key][0])\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "seq = [1,2,3,4,5,6,7,8,9,10]\n\nfor i in range(1, 10):\n    print(\"FOR LOOP\", i)\n    # split into input and output pair\n    in_seq, out_seq = seq[:i], seq[i]\n    in_seq = pad_sequences([in_seq], maxlen=52)[0]\n    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n    # pad input sequence\n    print(in_seq)\n    print(out_seq)\n    #print(f'length of in_seq {len(in_seq)}'))\n    #print(f'length of out_seq {len(out_seq)}'))",
   "metadata": {
    "tags": [],
    "cell_id": "00019-a2b86057-5777-42bd-87c8-c1e1cc633f64",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d67d5ca8-f99a-4baf-8748-4aa99efbd09b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "4c7d27de-9756-4001-aeca-4590ff924e2e",
  "deepnote_execution_queue": []
 }
}